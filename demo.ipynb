{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction using Linear Regression\n",
    "\n",
    "House price prediction is a common task in real estate and property markets. It involves estimating the value of a house based on various factors such as the number of bedrooms, the size of the house and other relevant features. Linear regression is a widely used statistical technique that can be applied to predict house prices based on these features.\n",
    "\n",
    "In this Python code, we will demonstrate how to build a simple linear regression model to predict house prices. We will use a dataset that contains information about different houses, including their features and corresponding prices. The goal is to train a regression model that can accurately estimate house prices based on the provided features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from src.linear_regression import GDLinearRegression, LinearRegression\n",
    "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
    "from src.tester import cross_validate, format_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The house price dataset contains 18 features from the year 2014 during the months from february to october. The dataset contains the relevant features for predicting the price which are :\n",
    "\n",
    "1) date : the date of the data for each house\n",
    "2) price : the actual price for the house\n",
    "3) bedrooms: the number of bedrooms in the house\n",
    "4) bathrooms: the number of bathrooms in the house\n",
    "5) sqft_living: the total living in square feet which is the area that will be heated or cooled or the area where you spend your time\n",
    "6) sqft_lot: the total lot in square feet which is the land you own according to the boundary lines determined by the city\n",
    "7) floors: the number of floors in the house\n",
    "8) waterfront: indicates whether the house has direct access to a natural or man-made waterway such as a lake, river, channel or canal.\n",
    "9) view: the view the house has\n",
    "10) condition: a numerical value that rates the condition of the house\n",
    "11) sqft_above: the total of all living square feet in a home that is above the ground.\n",
    "12) sqft_basement: the total square feet of the basement\n",
    "13) yr_built: the year the house was built in\n",
    "14) yr_renovated: the year the house was renovated in\n",
    "15) street: the address of the house and which street it is on\n",
    "16) city: the city the house is in\n",
    "17) statezip: the zip code for the state the house is in\n",
    "18) country: the country the house is located in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/house-price-data.csv\")\n",
    "df.head(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding city names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"city_counts\"] = df[\"city\"].map(df[\"city\"].value_counts())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier detection and removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(np.abs(stats.zscore(df[\"price\"])) < 2.9)]\n",
    "df = df[df[\"city_counts\"] > 100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature and label splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.loc[:, df.columns != \"price\"], df[\"price\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping unnecessary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop([\"date\", \"street\", \"statezip\", \"country\", \"city\"], axis=1, inplace=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.min()) / (X.max() - X.min())\n",
    "y = (y - y.min()) / (y.max() - y.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Preprocessing Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize models\n",
    "\n",
    "We built two models for this project. One is a simple linear regression model and the other is a linear regression model optimized using gradient descent. The two models will be compared to scikit-learn's linear regression model.\n",
    "\n",
    "### Least Squares Linear Regression\n",
    "\n",
    "The least squares linear regression model is a simple linear regression model that uses the ordinary least squares method to estimate the regression coefficients. The regression coefficients are represented by the Greek letter beta (Î²). The regression coefficients are used to estimate the target variable (y) using the following equation:\n",
    "\n",
    "\n",
    "$$y = \\beta X$$\n",
    "\n",
    "In matrix form, the variables are represented in the following way:\n",
    "\n",
    "$$\n",
    "y=\n",
    "\\begin{pmatrix}\n",
    "y_{1}\\\\\n",
    "y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "y_{n}\n",
    "\\end{pmatrix},\n",
    "\n",
    "\\beta=\n",
    "\\begin{pmatrix}\n",
    "\\beta_{0}\\\\\n",
    "\\beta_{1} \\\\\n",
    "\\beta_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_{p}\n",
    "\\end{pmatrix},\n",
    "X=\n",
    "\\begin{pmatrix}\n",
    "1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,p} \\\\\n",
    "1 & x_{2,1} & X_{2,2} & \\cdots & x_{2,p} \\\\\n",
    "\\vdots  & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "1 & x_{n,1} & x_{n,2} & \\cdots & x_{n,p} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$X^T.X$ is the covariance matrix of the features and $X^T.y$ is the covariance matrix of the features and the target variable. Their dot product is the covariance matrix of the features and the target variable. The inverse of the covariance matrix of the features is multiplied by the covariance matrix of the features and the target variable to obtain the regression coefficients.\n",
    "\n",
    "$$\\beta = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "#### Problems with one hot encoding\n",
    "\n",
    "When using least squares to fit a linear regression model, we get the inverse of the matrix $X^TX$. If the matrix $X^TX$ is not invertible, then the model cannot be fit using least squares. This is a problem when using one hot encoding to encode categorical features. One hot encoding creates a matrix with a column for each category. If there are too many categories, then the matrix $X^TX$ will not be invertible. This is known as the dummy variable trap. To avoid this problem, we can use a technique called one hot encoding with dummy variables. This technique creates a matrix with a column for each category minus one. This ensures that the matrix $X^TX$ is invertible.\n",
    "\n",
    "### Optimized Linear Regression using Gradient Descent\n",
    "\n",
    "Optimizing linear regression using gradient descent is about finding the optimal values for the regression coefficients that minimize the cost function by iteratively updating the regression coefficients in the direction of steepest descent. \n",
    "It contains hyperparameters such as: \n",
    "- Learning Rate: the step size taken in each iteration of gradient descent\n",
    "- Number of Iterations: the number that determines the maximum number of times gradient descent updates the regression coefficients.\n",
    "- The threshold: a value that acts as a stopping criterion for the gradient descent algorithm. Once the magnitude of the gradient vector falls   below the threshold, the algorithm terminates.\n",
    "\n",
    "$$\\beta = \\beta - \\alpha \\frac{2}{n}X^T(X\\beta - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"GDLinearRegression\": GDLinearRegression(\n",
    "        learning_rate=0.05, threshold=1e-9, max_iter=2500\n",
    "    ),\n",
    "    \"SklearnLinearRegression\": SklearnLinearRegression(),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test each model\n",
    "\n",
    "We cross-validate each model using 5-fold cross-validation and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    scores = cross_validate(5, model, X, y)\n",
    "    for k, v in scores.items():\n",
    "        print(f\"Model: {name}\\n - {k}: {format_score(v)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here, the optimized linear regression model performed slightly better than the simple linear regression model. However, it took a longer time to train the optimized linear regression model. However, the model is prone to errors in case the feature matrix is not invertible. This makes the optimized gradient descent model more reliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76d7c06053c3456e5600312cec90888656fc0ed30c03d8425b9dac6e4fc8e014"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
