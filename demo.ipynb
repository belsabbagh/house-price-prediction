{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction using Linear Regression\n",
    "\n",
    "House price prediction is a common task in real estate and property markets. It involves estimating the value of a house based on various factors such as the number of bedrooms, the size of the house and other relevant features. Linear regression is a widely used statistical technique that can be applied to predict house prices based on these features.\n",
    "\n",
    "In this Python code, we will demonstrate how to build a simple linear regression model to predict house prices. We will use a dataset that contains information about different houses, including their features and corresponding prices. The goal is to train a regression model that can accurately estimate house prices based on the provided features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from src.linear_regression import GDLinearRegression, LinearRegression\n",
    "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
    "from src.tester import cross_validate, format_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The house price dataset contains 18 features from the year 2014 during the months from february to october. The dataset contains the relevant features for predicting the price which are :\n",
    "\n",
    "1) date : the date of the data for each house\n",
    "2) price : the actual price for the house\n",
    "3) bedrooms: the number of bedrooms in the house\n",
    "4) bathrooms: the number of bathrooms in the house\n",
    "5) sqft_living: the total living in square feet which is the area that will be heated or cooled or the area where you spend your time\n",
    "6) sqft_lot: the total lot in square feet which is the land you own according to the boundary lines determined by the city\n",
    "7) floors: the number of floors in the house\n",
    "8) waterfront: indicates whether the house has direct access to a natural or man-made waterway such as a lake, river, channel or canal.\n",
    "9) view: the view the house has\n",
    "10) condition: a numerical value that rates the condition of the house\n",
    "11) sqft_above: the total of all living square feet in a home that is above the ground.\n",
    "12) sqft_basement: the total square feet of the basement\n",
    "13) yr_built: the year the house was built in\n",
    "14) yr_renovated: the year the house was renovated in\n",
    "15) street: the address of the house and which street it is on\n",
    "16) city: the city the house is in\n",
    "17) statezip: the zip code for the state the house is in\n",
    "18) country: the country the house is located in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/house-price-data.csv\")\n",
    "df.head(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Encoding city names\n",
    "To start preprocessing our data, we need to get rid of categorical values for the model to work, and to do that we start with the 'city' column that contains the names of the cities the houses are in. We change the city names to numerical values using the 'count or frequency encoding' preprocessing method, which takes each unique entry and gets its frequency (count) then replaces the categorical value with the new numerical value which represents the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "city_counts = df[\"city\"].map(df[\"city\"].value_counts())\n",
    "df.insert(len(df.columns), \"city_counts\", city_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection and removal\n",
    "In this section we remove outliers from the 'price' column by removing prices that are out of range, and to do that we use the 'zscore' method, then we detect the outliers in the new 'city_counts' column to remove the cities with frequency = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_distribution = np.abs(stats.zscore(df[\"price\"]))\n",
    "df = df[(price_distribution < 2.9) & (price_distribution > -2.9)]\n",
    "df = df[city_counts > 100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature and label splitting\n",
    "After encoding and detecting outliers we divide the data into features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.loc[:, df.columns != \"price\"], df[\"price\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unnecessary columns\n",
    "Then we drop the unnecessary columns that don't have an effect on the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop([\"date\", \"street\", \"statezip\", \"country\", \"city\"],\n",
    "           axis=1, inplace=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "After dropping unnecessary columns, we normalize the features and targets using the 'minmax' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.min()) / (X.max() - X.min())\n",
    "y = (y - y.min()) / (y.max() - y.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Preprocessing Techniques\n",
    "\n",
    "Frequency encoding was our choice for preprocessing, but we also tried using the 'one-hot-encoding' method for preprocessing the categorical values in the 'city' column, but it didn't work well with linear regression, and we'll explain why in details later in this document.\n",
    "- This was the function for loading the data and preprocessing it when we used 'one-hot-encoding'\n",
    "    ```python\n",
    "    def load_dataset():\n",
    "        df = pd.read_csv(\"data/house-price-data.csv\")\n",
    "        df['city'] = df['city'].astype('category')\n",
    "        df['city_enc'] = df['city'].cat.codes\n",
    "        enc = OneHotEncoder()\n",
    "        enc_data = pd.DataFrame(enc.fit_transform(df[['city_enc']]).toarray())\n",
    "        X, y = df.loc[:, df.columns != \"price\"], df[\"price\"]\n",
    "        X = X.drop([\"date\", \"street\", \"city\", \"statezip\", \"country\"], axis=1, inplace=False)\n",
    "        X = (X - X.mean()) / X.std()\n",
    "        y = (y - y.mean()) / y.std()\n",
    "        X = X.join(enc_data)\n",
    "        return X, y\n",
    "    ```\n",
    "- We also changed the method for normalization from 'z-score' method to 'min-max' method, and found that the 'min-max' scoring is better than 'z-score'. We found that 'min-max' doesn't affect 'one-hot-encoding' and this fixed the problem we had with 'z-score'.\n",
    "-- This is the previous code for 'z-score':\n",
    "    ```python\n",
    "    X = (X - X.mean()) / X.std()\n",
    "    y = (y - y.mean()) / y.std()\n",
    "    ```\n",
    "\n",
    "These are the results we got with 'z-score' preprocessing method:\n",
    "- With LinearRegression: mse: 0.48617 +/- 0.07278\n",
    "- With GDLinearRegression: mse: 0.48617 +/- 0.07278\n",
    "- With SklearnLinearRegression: mse: 35824058772341710848.00000 +/- 71648117544683421696.00000, we faced a problem with the Sklearn model when using the frequency encoding method with 'z-score', and we will explain the difference between our models later in the document.\n",
    "\n",
    "And these are the results we got when we used 'min-max' preprocessing method:\n",
    "- With LinearRegression: mse: 0.00888 +/- 0.00133\n",
    "- With GDLinearRegression: mse: 0.00888 +/- 0.00133\n",
    "- With SklearnLinearRegression: mse: 0.00888 +/- 0.00133\n",
    "\n",
    "As shown above, we found that the 'min-max' method outperforms the 'z-score' method and gives smaller mse values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize models\n",
    "\n",
    "We built two models for this project. One is a simple linear regression model and the other is a linear regression model optimized using gradient descent. The two models will be compared to scikit-learn's linear regression model.\n",
    "\n",
    "### Least Squares Linear Regression\n",
    "\n",
    "The least squares linear regression model is a simple linear regression model that uses the ordinary least squares method to estimate the regression coefficients. The regression coefficients are represented by the Greek letter beta (Î²). The regression coefficients are used to estimate the target variable (y) using the following equation:\n",
    "\n",
    "\n",
    "$$y = \\beta X$$\n",
    "\n",
    "In matrix form, the variables are represented in the following way:\n",
    "\n",
    "$$\n",
    "y=\n",
    "\\begin{pmatrix}\n",
    "y_{1}\\\\\n",
    "y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "y_{n}\n",
    "\\end{pmatrix},\n",
    "\n",
    "\\beta=\n",
    "\\begin{pmatrix}\n",
    "\\beta_{0}\\\\\n",
    "\\beta_{1} \\\\\n",
    "\\beta_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_{p}\n",
    "\\end{pmatrix},\n",
    "X=\n",
    "\\begin{pmatrix}\n",
    "1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,p} \\\\\n",
    "1 & x_{2,1} & X_{2,2} & \\cdots & x_{2,p} \\\\\n",
    "\\vdots  & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "1 & x_{n,1} & x_{n,2} & \\cdots & x_{n,p} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$X^T.X$ is the covariance matrix of the features and $X^T.y$ is the covariance matrix of the features and the target variable. Their dot product is the covariance matrix of the features and the target variable. The inverse of the covariance matrix of the features is multiplied by the covariance matrix of the features and the target variable to obtain the regression coefficients.\n",
    "\n",
    "$$\\beta = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "#### Problems with one hot encoding\n",
    "\n",
    "When using least squares to fit a linear regression model, we get the inverse of the matrix $X^TX$. If the matrix $X^TX$ is not invertible, then the model cannot be fit using least squares. This is a problem when using one hot encoding to encode categorical features. One hot encoding creates a matrix with a column for each category. If there are too many categories, then the matrix $X^TX$ will not be invertible. This is known as the dummy variable trap. To avoid this problem, we can use a technique called one hot encoding with dummy variables. This technique creates a matrix with a column for each category minus one. This ensures that the matrix $X^TX$ is invertible.\n",
    "\n",
    "### Optimized Linear Regression using Gradient Descent\n",
    "\n",
    "Optimizing linear regression using gradient descent is about finding the optimal values for the regression coefficients that minimize the cost function by iteratively updating the regression coefficients in the direction of steepest descent. \n",
    "It contains hyperparameters such as: \n",
    "- Learning Rate: the step size taken in each iteration of gradient descent\n",
    "- Number of Iterations: the number that determines the maximum number of times gradient descent updates the regression coefficients.\n",
    "- The threshold: a value that acts as a stopping criterion for the gradient descent algorithm. Once the magnitude of the gradient vector falls   below the threshold, the algorithm terminates.\n",
    "\n",
    "$$\\beta = \\beta - \\alpha \\frac{2}{n}X^T(X\\beta - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"GDLinearRegression\": GDLinearRegression(\n",
    "        learning_rate=0.05, threshold=1e-9, max_iter=2500\n",
    "    ),\n",
    "    \"SklearnLinearRegression\": SklearnLinearRegression(),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test each model\n",
    "\n",
    "We cross-validate each model using 5-fold cross-validation and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    scores = cross_validate(5, model, X, y)\n",
    "    for k, v in scores.items():\n",
    "        print(f\"Model: {name}\\n - {k}: {format_score(v)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here, the optimized linear regression model performed slightly better than the simple linear regression model. However, it took a longer time to train the optimized linear regression model. However, the model is prone to errors in case the feature matrix is not invertible. This makes the optimized gradient descent model more reliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76d7c06053c3456e5600312cec90888656fc0ed30c03d8425b9dac6e4fc8e014"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
